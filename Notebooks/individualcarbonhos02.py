# -*- coding: utf-8 -*-
"""IndividualCarbonHOS02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZdBS7ib1ntz3toJuh3mHB3TvGNeb6NOK
"""



"""### I went back to HOS02 because I felt like I understood that assignment the best. I based this loosely on HOS02 however I used the pipeline.

I could not get the scaling from HOS02 to work for our dataset even after changing the scalers for objects instead of numbers:


x_scaler = StandardScaler()
x_encoder = OneHotEncoder(drop='first')
preprocessor = ColumnTransformer([('scaler', x_scaler, make_column_selector(dtype_include= "number")),
                                  ('encoder', x_encoder, make_column_selector(dtype_include= "object"))])


y_scaler = StandardScaler()

### This code has been replaced with a pipeline and it is working. However I can only get one model to work with is KNeighborsRegressor. I cannot get the Gaussian, Logistic Regressiong or KNeighborsClassifier to work.



"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# import the data
df = pd.read_csv('Carbon Emission.csv')
df.head()

df.info()

# drop the rows with missing values
# df = df.dropna()
# this does not work wth our data as it drops the whole dataset
# lets list the columns with null values
df.isnull().sum()

# lets just drop Vehicle Type column
df = df.drop('Vehicle Type', axis=1)
df.head()

# split the dataset into test/train
# this creams a new dataframe with the target variable removed
x = df.drop(columns=['CarbonEmission'])
# this creates a new dataframe with only the target variable
y = df[['CarbonEmission']]
# split the data into test/train

## Commenting this out because I think we need to use the pipeline to do this

# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=620)

# print(x_train.shape, y_train.shape)
# print(x_test.shape, y_test.shape)

"""### This is not working yet disregard"""

# this preprocessor is Not working. Commented out.
# # turn the train/test into dataframes? Arent they already dataframes?
# print(type(x_train))
# # they are already dataframes

# # preprocessor for x
# x_scaler = StandardScaler()
# x_encoder = OneHotEncoder(drop='first')
# preprocessor = ColumnTransformer([('scaler', x_scaler, make_column_selector(dtype_include= "number")),
#                                   ('encoder', x_encoder, make_column_selector(dtype_include= "object"))])

# # preprocessor for y
# # remember the y is a number so we only need to scale it, we do not need to encode
# y_scaler = StandardScaler()

# # transform the data
# x_train = preprocessor.fit_transform(x_train)
# y_train = y_scaler.fit_transform(y_train).reshape(y_train.shape[0])

# x_test = preprocessor.transform(x_test)
# y_test = y_scaler.transform(y_test).reshape(y_test.shape[0])

# print("-------------------X-------------------")
# print(x)
print("-------------------Y-------------------")
print(y)

# Data looks good at this point

# ok this data set has a lot of caterogiocal and numerical data
# lets pull out the categorical and numerical seperately
categorical = x.select_dtypes(include=['object']).columns
print(categorical)

# do the same for numerical
numerical = x.select_dtypes(include=['int64']).columns
print(numerical)

# for the numerical data lets use StandardScaler with SimpleImputer
# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
# StandardScaler will Standardize features by removing the mean and scaling to unit variance.
# https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html
# Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value.
numerical_transformer = Pipeline(steps=[
    # strategy = mean, median, most_frequent, constant
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# for the categorical data lets use OneHotEncoder with SimpleImputer
#
# this will Encode categorical features as a one-hot numeric array.
# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    # handle_unknown could be error, ignore, or constant
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Lets preprocess the data using the transformers we created
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical),
        ('cat', categorical_transformer, categorical)
    ])

# split the data into test/train
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# check the shape
print(x_train.shape, y_train.shape)

"""### Cannot get the Other 3 models to work. Uncomment and try it out. Getting error message about expecting 2d and getting 1d?"""

# lets check 3 models
# checking 3 diffrent models
# models = [LogisticRegression(), GaussianNB(), KNeighborsClassifier()]
# for model in models:
#     clf = Pipeline(steps=[('preprocessor', preprocessor),
#                           ('classifier', model)])
#     clf.fit(x_train, y_train)
#     print(model)
#     print("model score: %.3f" % clf.score(x_test, y_test))
#     print("cross validation score: %.3f" % np.mean(cross_val_score(clf, x, y, cv=5)))
#     print()
# this didnt work, lets try another approach

model = KNeighborsRegressor(n_neighbors=5)
# lets just try one model
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# Fit the model
clf.fit(x_train, y_train)

# Make predictions
y_pred = clf.predict(x_test)

# Evaluate the model
score = clf.score(x_test, y_test)
print(f'Model score: {score}')

"""### Another Attempt at the other models from HOS02. Still not working"""

# lets try the other models
# models = [LogisticRegression(), GaussianNB(), KNeighborsClassifier()]
# for model in models:
#     clf2 = Pipeline(steps=[('preprocessor', preprocessor),
#                           ('classifier', model)])
#     clf2.fit(x_train, y_train)
#     print(model)
#     score = clf.score(x_test, y_test)
#     print(f'Model score: {score}')

# I still cannot get these other models to work

# lets use our working model to make a prediction
# here we have a person

# get a random row from the dataset
person = x.sample()
# get thei carbon emission
carbon_emission = y.loc[person.index]
# predict the carbon emission
predicted_carbon_emission = clf.predict(person)
print(f'Actual Carbon Emission: {carbon_emission.values[0]}')
print(f'Predicted Carbon Emission: {predicted_carbon_emission[0]}')

# wow that seams really good
# lets do that in a loop and see if we just got lucky
for i in range(5):
    person = x.sample()
    carbon_emission = y.loc[person.index]
    predicted_carbon_emission = clf.predict(person)
    print(f'Actual Carbon Emission: {carbon_emission.values[0]}')
    print(f'Predicted Carbon Emission: {predicted_carbon_emission[0]}')
    print()

# ok lets try to improve the model
# lets mess with the n_neighbors parameter
for i in range(10):
    model = KNeighborsRegressor(n_neighbors=i+1)
    # lets just try one model
    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Fit the model
    clf.fit(x_train, y_train)

    # Make predictions
    y_pred = clf.predict(x_test)

    # Evaluate the model
    score = clf.score(x_test, y_test)
    print(f'k={i+1}, score={score}')

# it looks like 7 is the best k value?
# lets run our test agwain with k=7
model = KNeighborsRegressor(n_neighbors=7)

clf3 = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# Fit the model
clf3.fit(x_train, y_train)

# Make predictions
for i in range(10):
    person = x.sample()
    carbon_emission = y.loc[person.index]
    predicted_carbon_emission = clf.predict(person)
    print(f'Actual Carbon Emission: {carbon_emission.values[0]}')
    print(f'Predicted Carbon Emission: {predicted_carbon_emission[0]}')
    print()


import joblib

# Save the model to disk
joblib.dump(clf3, 'carbon_emission_predictor.pkl')

# Load the model from disk
clf_loaded = joblib.load('carbon_emission_predictor.pkl')
# Example of making a prediction with the loaded model
person = x.sample()
predicted_carbon_emission = clf_loaded.predict(person)
print(f'Predicted Carbon Emission: {predicted_carbon_emission[0]}')